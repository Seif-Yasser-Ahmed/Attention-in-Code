{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05197d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628be321",
   "metadata": {},
   "source": [
    "# Self Attention (Encoder Only Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3caf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"write a poem <EOS>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99fac89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
    "        super().__init__()\n",
    "        self.W_q=nn.Linear(d_model, d_model,bias=False) #Query\n",
    "        self.W_k=nn.Linear(d_model, d_model,bias=False) #Key\n",
    "        self.W_v=nn.Linear(d_model, d_model,bias=False) #Value\n",
    "        self.row_dim=row_dim\n",
    "        self.col_dim=col_dim\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        q=self.W_q(token_embeddings)\n",
    "        k=self.W_k(token_embeddings)\n",
    "        v=self.W_v(token_embeddings)\n",
    "\n",
    "        sims=torch.matmul(q,k.transpose(dim0=self.row_dim,dim1=self.col_dim))\n",
    "        scaled_sims=sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        attention_percents=F.softmax(scaled_sims,dim=self.col_dim)\n",
    "        attention_output=torch.matmul(attention_percents,v)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acf4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_matrix=torch.tensor([[1.16,.23],\n",
    "              [.57,1.36],\n",
    "              [4.41,-2.16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "612fee00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x265ffaac5b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644552fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SelfAttention_model=SelfAttention(d_model=2,row_dim=0,col_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c214601e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelfAttention_model(encodings_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c6bcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5406,  0.5869],\n",
      "        [-0.1657,  0.6496]], requires_grad=True)\n",
      "tensor([[ 0.5406, -0.1657],\n",
      "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(SelfAttention_model.W_q.weight)\n",
    "print(SelfAttention_model.W_q.weight.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c835946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b4041d",
   "metadata": {},
   "source": [
    "# Masked Self Attention (Decoder Only Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "772577a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self,d_model=2,row_dim=0,col_dim=1):\n",
    "        super().__init__()\n",
    "        self.W_q=nn.Linear(d_model, d_model,bias=False) #Query\n",
    "        self.W_k=nn.Linear(d_model, d_model,bias=False) #Key\n",
    "        self.W_v=nn.Linear(d_model, d_model,bias=False) #Value\n",
    "        self.row_dim=row_dim\n",
    "        self.col_dim=col_dim\n",
    "\n",
    "\n",
    "    def forward(self,token_encodings,mask=None):\n",
    "        q=self.W_q(token_encodings)\n",
    "        k=self.W_k(token_encodings)\n",
    "        v=self.W_v(token_encodings)\n",
    "        sims=torch.matmul(q,k.transpose(dim0=self.row_dim,dim1=self.col_dim))\n",
    "        scaled_sims=sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        if mask is not None:\n",
    "            scaled_sims=scaled_sims.masked_fill(mask=mask,value=float('-inf'))\n",
    "        attention_percents=F.softmax(scaled_sims,dim=self.col_dim)\n",
    "        attention_output=torch.matmul(attention_percents,v)\n",
    "        return attention_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0fc6e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28ac323b190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "378457c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_matrix=torch.tensor([[1.16,.23],\n",
    "              [.57,1.36],\n",
    "              [4.41,-2.16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f79ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskAttention_model=MaskedSelfAttention(d_model=2,row_dim=0,col_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b90eef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=torch.tril(torch.ones(3,3),diagonal=0)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59abf2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True],\n",
       "        [False, False,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc0a65fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.0062,  0.6072],\n",
       "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maskAttention_model(encodings_matrix,mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470de89",
   "metadata": {},
   "source": [
    "# Cross (Encoder-Decoder) Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae315be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,d_model=2,row_dim=0,col_dim=1):\n",
    "        super().__init__()\n",
    "        self.W_q=nn.Linear(d_model, d_model,bias=False) #Query\n",
    "        self.W_k=nn.Linear(d_model, d_model,bias=False) #Key\n",
    "        self.W_v=nn.Linear(d_model, d_model,bias=False) #Value\n",
    "        self.row_dim=row_dim\n",
    "        self.col_dim=col_dim\n",
    "\n",
    "    def forward(self,encodings_for_q,encodings_for_k,encodings_for_v,mask=None):\n",
    "        q=self.W_q(encodings_for_q)\n",
    "        k=self.W_k(encodings_for_k)\n",
    "        v=self.W_v(encodings_for_v)\n",
    "        sims=torch.matmul(q,k.transpose(dim0=self.row_dim,dim1=self.col_dim))\n",
    "        scaled_sims=sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "        if mask is not None:\n",
    "            scaled_sims=scaled_sims.masked_fill(mask=mask,value=float('-inf'))\n",
    "        attention_percents=F.softmax(scaled_sims,dim=self.col_dim)\n",
    "        attention_output=torch.matmul(attention_percents,v)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4eb8c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_for_q=torch.tensor([[1.16,.23],\n",
    "              [.57,1.36],\n",
    "              [4.41,-2.16]])\n",
    "encodings_for_k=torch.tensor([[1.16,.23],\n",
    "              [.57,1.36],\n",
    "              [4.41,-2.16]])\n",
    "encodings_for_v=torch.tensor([[1.16,.23],\n",
    "              [.57,1.36],\n",
    "              [4.41,-2.16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c86ab4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28ac323b190>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb7c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention=Attention(d_model=2,row_dim=0,col_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64025605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100, 1.0641],\n",
       "        [0.2040, 0.7057],\n",
       "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(encodings_for_q,encodings_for_k,encodings_for_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fa77f",
   "metadata": {},
   "source": [
    "# Multi-Head Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0a8ceb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model=2,num_heads=1,row_dim=0,col_dim=1):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Attention(d_model=d_model,row_dim=row_dim,col_dim=col_dim) for _ in range(num_heads)])\n",
    "        self.col_dim=col_dim\n",
    "\n",
    "    def forward(self,encodings_for_q,encodings_for_k,encodings_for_v):\n",
    "        return torch.cat([head(encodings_for_q,encodings_for_k,encodings_for_v) for head in self.heads],dim=self.col_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78db8c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28ac323b190>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2656743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiheadAttention_model=MultiHeadAttention(d_model=2,num_heads=2,row_dim=0,col_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96e10c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_for_q=torch.tensor([[1.16,.23],\n",
    "                [.57,1.36],\n",
    "                [4.41,-2.16]])\n",
    "encodings_for_k=torch.tensor([[1.16,.23],\n",
    "                [.57,1.36],\n",
    "                [4.41,-2.16]])\n",
    "encodings_for_v=torch.tensor([[1.16,.23],\n",
    "                [.57,1.36],\n",
    "                [4.41,-2.16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb1b2797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
       "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
       "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiheadAttention_model(encodings_for_q,encodings_for_k,encodings_for_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e630b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
